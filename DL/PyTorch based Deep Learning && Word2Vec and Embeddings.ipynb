{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb37ff6a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Word2Vec provides to obtain not just some vectors of words but vectors saving the semantics of the words. A wonderful property of saved semantics of the words is in obtained embeddings space the words with similar sence are close to each other. Moreover, the words which appear in similar <b>contexts</b> (such as tea, coffee, water, etc.) are also close to each other in the same space (but in some different way). Different words are quite far from each other in the embeddings space. So, as a metric of closeness of words to each other or of a metric of a common context we can use distance in embedding space in this approach.\n",
    "\n",
    "Word2Vec has 2 method of implementation:\n",
    "* <b>CBOW</b> (Continuous Bag-Of-Words). By a context (words around) of the word we are trying to predict the central word.\n",
    "* <b>Skip-gram</b>. By the central word we are trying to predict words from a context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd2763c",
   "metadata": {},
   "source": [
    "<center><img src=\"CBOW.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6585c1d1",
   "metadata": {},
   "source": [
    "Practice demonstrates that <b>Skip-gram</b> architecture works better than <b>CBOW</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5514dd0",
   "metadata": {},
   "source": [
    "# Data downloading and data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8e80d9",
   "metadata": {},
   "source": [
    "## Downloading\n",
    "\n",
    "This text is a file of a purified file text from Wikipedia from Matt Mahoni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc72e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdffaa5",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1185de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./text8/text8\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f85395",
   "metadata": {},
   "source": [
    "### Purifying of data\n",
    "\n",
    "Firstly, we need to purify our text. We need to:\n",
    "* Transfrom any punctuation marks to tokens, that's why we exchange dots to \\<PERIOD\\>.\n",
    "* Delete any words which have ever met in the text five of less times. It must strongly decrease noise in our data and must strongly increase quality of the model.\n",
    "* (optionally) Have possibility of returning of list of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094daccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove all words with  5 or fewer occurences\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > 5]\n",
    "\n",
    "    return trimmed_words\n",
    "\n",
    "\n",
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: Two dictionaries, vocab_to_int, int_to_vocab\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    # sorting the words from most to least frequent in text occurrence\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    # create int_to_vocab dictionaries\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = preprocess(text)\n",
    "print(f\"Belonging to the text: {words[:30]}\")\n",
    "print(f\"Total words in the text: {len(words)}\")\n",
    "print(f\"Unique words: {len(set(words))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8578a93b",
   "metadata": {},
   "source": [
    "### Mapping of words to indices\n",
    "\n",
    "Here we create two dictionaries for transforming words to integer numbers and reverse. Integer numbers are assigned in order to decreasing of frequency, that's why to the most frequent word (the) is assigned number 0, the next word has number -1 and so on.\n",
    "\n",
    "Then when we have dictionaries, words are transformed into numbers and saved into `int_words` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3096d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "print(int_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9502531b",
   "metadata": {},
   "source": [
    "### Subsampling\n",
    "\n",
    "Frequently encountered words such as \"the\", \"of\", \"for\" not provide a significant context to closely placed words. If we throw out some of them, we will be able to delete a piece of noise from our data and obtain faster training of the model and better representation. This process is sometimes called <b>subsmapling</b>. For each word $\\omega_i$ in training sample we throw out it with probability equals\n",
    "\n",
    "$$P(\\omega_i) = 1 - \\sqrt{\\frac{t}{f(\\omega_i)}}$$\n",
    "\n",
    "where $t$ - a threshold parameter, $f(\\omega_i)$ - frequency of the word $\\omega_i$ in the whole set of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f39573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words) # dictionary with number of appearences for each word\n",
    "print(f\"42-th word appears in the text {word_counts[42]} times\")\n",
    "\n",
    "train_words = []\n",
    "\n",
    "for int_w in tqdm(int_words):\n",
    "    if random.random() < (threshold / (word_counts[int_w] / len(int_words))) ** 0.5:\n",
    "        train_words.append(int_w)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029906cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int_words[:15])\n",
    "print(train_words[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c392c36",
   "metadata": {},
   "source": [
    "### Making of training couples\n",
    "\n",
    "Now, after preliminary data preprocessing we need to make training couples to train the model. In skip-gram architecture for each word in the text we want to define envioroning context and take all the words in the window of size $C$ around this word.\n",
    "\n",
    "But it's reasonable to hold $C$ unfixed. It helps hold the model undependent from size of the window. If we, for instance, take $C = 5$, then for each train word we choose randomly some number $R$ in an interval $[1: C]$ and thn use $R$ as size of the window (use $R$ next and $R$ previous words as right labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ee14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def get_target(words: List[int], idx: int, window_size: int=5) -> List[int]:\n",
    "    \"\"\"\n",
    "    words: a text represented as a sequence of words indices\n",
    "    idx: index of the central word that is used to make a batch\n",
    "    window_size: controls the size of the window for each word\n",
    "    \n",
    "    returns list of words in a window of a window_size size\n",
    "    \"\"\"\n",
    "    \n",
    "    r = random.randint(1, window_size)\n",
    "    target = words[max(0, idx - r): idx] + words[idx + 1: min(len(words), idx + r)]\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text = [i for i in range(10)]\n",
    "idx = 5\n",
    "target = get_target(int_text, idx=idx, window_size=5)\n",
    "print(\"Input: \", int_text)\n",
    "print(f\"Index of interest: {idx}\")\n",
    "print(\"Target: \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007115d",
   "metadata": {},
   "source": [
    "### Generating batches for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f779db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words: List[int], batch_size: int, window_size=5) -> Tuple[List[int], List[int]]:\n",
    "    \n",
    "    for i in range(0, len(words) // batch_size * batch_size, batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[i: i + batch_size]\n",
    "        for j in range(len(batch)):\n",
    "            batch_x = batch[j]\n",
    "            batch_y = get_target(batch, j, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x] * len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text = [i for i in range(20)]\n",
    "\n",
    "batch_gen = get_batches(int_text, batch_size=4, window_size=5)\n",
    "x, y = next(batch_gen)\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86886642",
   "metadata": {},
   "source": [
    "# Skip-gram training\n",
    "\n",
    "Below you can see a chematic representation of our network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65febb7",
   "metadata": {},
   "source": [
    "<center><img src=\"skip-gram.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c9c378",
   "metadata": {},
   "source": [
    "* Input words are transmitted as batches of indices of input words. Target variables for each input word - word number from the context.\n",
    "* Input words is processed by linear layer `vocab_size` x `embed_size`.\n",
    "* Obtained embeddings is processed by output linear layer of size `embed_size` x `vocab_size` and loss for classification problems is applyed to outputs.\n",
    "\n",
    "The main idea of the method is training of a weights matrix of embeddings layer and searching for effective representations of our layers. After training we can throw out softmax layer because we don't need to make predictions based on this network we just need embeddings matrix to use it in another networks which we build with using of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f3cb6",
   "metadata": {},
   "source": [
    "<b>A couple of words about learning control - validation</b>.\n",
    "\n",
    "We need to choose a few unusual words. Then we can print the neares to them using cosine similarity:\n",
    "$$similarity = cos(\\theta) = \\frac{\\overrightarrow{a} \\cdot \\overrightarrow{b}}{|\\overrightarrow{a}||\\overrightarrow{b}|}$$\n",
    "\n",
    "We can encode the words of validation dataset as vectors $\\overrightarrow{a}$ by using of embeddings table and then calculate similarity to each word vector $\\overrightarrow{b}$ in embeddings table. Having similarity we can print the validation words and the words our embeddings matrix semantically similar to these words. This is a good way to check whether our embeddings table join the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3007a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size // 2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                              random.sample(range(1000, 1000+valid_window), valid_size // 2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t()) / magnitudes\n",
    "    \n",
    "    return valid_examples, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f59b1",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9863f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_size: int):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        out = self.linear(self.embed(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5537f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print_every = 200\n",
    "steps = 0\n",
    "n_epochs = 5\n",
    "batch_size = 1024\n",
    "embedding_dim = 128\n",
    "\n",
    "model = SkipGram(len(vocab_to_int), embedding_dim)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "for e in trange(n_epochs, leave=True, desc=\"Epoch number\"):\n",
    "    pbar = tqdm(\n",
    "        get_batches(train_words, batch_size),\n",
    "        leave=False,\n",
    "        desc=\"Batch number\",\n",
    "        total=len(train_words) // batch_size\n",
    "    )\n",
    "    \n",
    "    for inputs, targets in pbar:\n",
    "        steps+=1\n",
    "        inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        log_ps = model(inputs)\n",
    "        loss = criterion(log_ps, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            valid_examples, valid_similarities = cosine_similarity(model.embed, device=device)\n",
    "            _, closest_idxs = valid_similarities.topk(6)\n",
    "            \n",
    "            valid_examples, closest_idxs = valid_examples.to(\"cpu\"), closest_idxs.to(\"cpu\")\n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "                print(int_to_vocab[valid_idx.item()] + \" | \" + \", \".join(closest_words))\n",
    "            print(\"...\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca172fe",
   "metadata": {},
   "source": [
    "# Analyzing of trained model\n",
    "\n",
    "We use T-SNE to visualize how our multidimensional words vercors are grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9221b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embed.weight.to(\"cpu\").data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_words = 600\n",
    "tsne= TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf618b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color=\"steelblue\")\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
