{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W8R8WgZceEk"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Let's consider an example of automatical genration of texts with using of Recurrent Neural Network (RNN).\n",
    "\n",
    "* Useful meterials about RNN: [here](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* PyTorch implementation: [here](https://github.com/karpathy/char-rnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sqUOE2flceEl"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wHfCDyzceEl"
   },
   "source": [
    "# Data preparation\n",
    "\n",
    "## Anna Karenina text loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b34kfqIOceEl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"anna.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iC21bopceEl"
   },
   "source": [
    "## Tokenization of the text\n",
    "\n",
    "Let's make two dictionaries for transformation of symbols to integer numbers and reverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tYVlmnxLceEl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29, 10,  8, 56, 14, 38, 74,  4, 31, 37, 37, 37,  3,  8, 56, 56, 13,  4,\n",
       "        15,  8, 35, 77, 11, 77, 38, 30,  4,  8, 74, 38,  4,  8, 11, 11,  4,  8,\n",
       "        11, 77, 75, 38, 22,  4, 38, 58, 38, 74, 13,  4, 64, 44, 10,  8, 56, 56,\n",
       "        13,  4, 15,  8, 35, 77, 11, 13,  4, 77, 30,  4, 64, 44, 10,  8, 56, 56,\n",
       "        13,  4, 77, 44,  4, 77, 14, 30,  4, 26, 61, 44, 37, 61,  8, 13, 24, 37,\n",
       "        37, 17, 58, 38, 74, 13, 14, 10, 77, 44])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chars = tuple(set(text))\n",
    "int2char = dict(enumerate(unique_chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = torch.tensor([char2int[ch] for ch in text])\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azltQy-gceEl"
   },
   "source": [
    "Let's take a look on the scheme of char-RNN:\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charseq.jpeg?raw=1\" width=\"30%\">\n",
    "\n",
    "The network waits **one-hot encoded** input. Below you can find the function to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OnahALhiceEl"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(int_words: torch.Tensor, n_labels: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates one-hot representation matrix for a given batch of integer sequences\n",
    "    :param int_words: tensor of ints, which represents current sequence; shape: [batch_size, seq_len]\n",
    "    :param n_labels: vocabulary size (number of unique tokens in data)\n",
    "    :return: one-hot representation of the input tensor; shape: [batch_size, seq_len, n_labels]\n",
    "    \"\"\"\n",
    "    words_one_hot = torch.zeros(\n",
    "        (int_words.numel(), n_labels), dtype=torch.float32, device=int_words.device\n",
    "    )\n",
    "    words_one_hot[torch.arange(words_one_hot.shape[0]), int_words.flatten()] = 1.0\n",
    "    words_one_hot = words_one_hot.reshape((*int_words.shape, n_labels))\n",
    "\n",
    "    return words_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# testing the function\n",
    "test_seq = torch.tensor([[3, 5, 1], [0, 2, 4]])\n",
    "test_one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(test_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YyL91CuceEl"
   },
   "source": [
    "## Batches making\n",
    "\n",
    "As a simple example, batches looks like the following: we take encoded symbols and divide it into several sequences defined by a parameter `batch_size`. Each of our sequences has length `seq_length`.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/sequence_batching@1x.png?raw=1\" width=500px>\n",
    "\n",
    "**1. Throwing out a piece of the text to make \"whole\" batches.**\n",
    "\n",
    "Each batch contains $N \\times M$ symbols where $N$ is number of sequences in batch (`batch_size`), $M$ — length of each sequence (`seq_length`). Then to obtain number of batches $K$ which we can make from the text, it's needed to divide length of the initial sequence by number of symbols in a batch. Then we can find the number of symbols which we should save in the text to form of $K$ batches: $N \\times M \\times K$.\n",
    "\n",
    "**2. Dividing of the text into $N$ pieces**\n",
    "\n",
    "This step is needed to have a possibility to going among the text by a window of size `[batch_size, seq_len]`. It can be implemented with simple `reshape`.\n",
    "\n",
    "**3. When we have a text matrix we can move along it with the window to obtain batches**\n",
    "\n",
    "From each position of the window we make training couples `(x, y)` the following way: $x$ - is all the elements of the window except the last column, $y$ - is all the lements of the window except the first column. Thereby for each token of initial text we will make prediction of the token following by this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2ECftYejnvpx"
   },
   "outputs": [],
   "source": [
    "def get_batches(\n",
    "    int_words: torch.Tensor, batch_size: int, seq_length: int\n",
    ") -> Iterable[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generates batches from encoded sequence.\n",
    "    :param int_words: tensor of ints, which represents the text; shape: [batch_size, -1]\n",
    "    :param batch_size: number of sequences per batch\n",
    "    :param seq_length: number of encoded chars in a sequence\n",
    "    :return: generator of pairs (x, y); x_shape, y_shape: [batch_size, seq_length - 1]\n",
    "    \"\"\"\n",
    "    # 1. Truncate text, so there are only full batches\n",
    "    window_size = seq_length + 1\n",
    "    batch_size_total = batch_size * window_size\n",
    "    n_batches = len(int_words) // batch_size_total\n",
    "    int_words = int_words[: n_batches * batch_size_total]\n",
    "\n",
    "    # 2. Reshape into batch_size rows\n",
    "    int_words = int_words.reshape((batch_size, -1))\n",
    "\n",
    "    # 3. Iterate through the text matrix\n",
    "    for position in range(0, int_words.shape[1], window_size):\n",
    "        x = int_words[:, position : position + window_size - 1]\n",
    "        y = int_words[:, position + 1 : position + window_size]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qtKlLXi1ceEl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tensor([[29, 10,  8, 56, 14, 38, 74,  4, 31, 37],\n",
      "        [56, 56, 38, 45,  4,  8, 44, 45,  4, 30],\n",
      "        [14, 10,  8, 14,  4, 77, 44, 30, 26, 11],\n",
      "        [38, 61,  4, 14, 10, 26, 64, 60, 10, 14],\n",
      "        [38, 74, 73,  4, 10, 38,  4, 10,  8, 45],\n",
      "        [74, 73,  4, 61, 10, 26,  4, 61,  8, 30],\n",
      "        [30, 14,  4, 43, 38,  4, 76, 26, 58, 38],\n",
      "        [73,  4, 23, 11, 38, 79, 38, 13,  4, 23]])\n",
      "\n",
      "y:\n",
      "tensor([[10,  8, 56, 14, 38, 74,  4, 31, 37, 37],\n",
      "        [56, 38, 45,  4,  8, 44, 45,  4, 30, 10],\n",
      "        [10,  8, 14,  4, 77, 44, 30, 26, 11, 64],\n",
      "        [61,  4, 14, 10, 26, 64, 60, 10, 14, 15],\n",
      "        [74, 73,  4, 10, 38,  4, 10,  8, 45,  4],\n",
      "        [73,  4, 61, 10, 26,  4, 61,  8, 30,  4],\n",
      "        [14,  4, 43, 38,  4, 76, 26, 58, 38, 74],\n",
      "        [ 4, 23, 11, 38, 79, 38, 13,  4, 23, 11]])\n"
     ]
    }
   ],
   "source": [
    "# testing the function\n",
    "test_batches = get_batches(encoded, 8, 50)\n",
    "test_x, test_y = next(test_batches)\n",
    "assert test_x.shape == test_y.shape\n",
    "print(f\"x:\\n{test_x[:10, :10]}\\n\")\n",
    "print(f\"y:\\n{test_y[:10, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnaData(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, int_words: torch.Tensor, batch_size: int, seq_length: int):\n",
    "        self.int_words = int_words\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        return get_batches(self.int_words, self.batch_size, self.seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jouxv0L2ceEl"
   },
   "source": [
    "# Model implementation\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charRNN.png?raw=1\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7s5eRaoceEl"
   },
   "source": [
    "## Structure of the model\n",
    "\n",
    "* Making and storing required dictionaries.\n",
    "* Defining of layer [LSTM]((https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)) with class `torch.nn.LSTM` instance, which takes parameters set: `input_size` — length of the sequence in a batch; `n_hidden` — size of the hidden layers; `n_layers` — number of the layers; `drop_prob` — dropout probability; и `batch_first` — a flag marking the fact that input sequences have batch dimension along the zero axis.\n",
    "* Defining of Dropout layer with the same value `drop_prob`.\n",
    "* Defining of fully connected layer with set of parameters: dimension of input — `n_hidden`; dimesion of output — dictionary size.\n",
    "* And, finally, initializing of the weights and initial hidden state (`self.init_hidden()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VPq1EA38rBqn"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unique_tokens: Tuple[str],\n",
    "        n_hidden: int = 256,\n",
    "        n_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        # create mappings\n",
    "        self.unique_tokens = unique_tokens\n",
    "        self.int2char = dict(enumerate(self.unique_tokens))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "\n",
    "        ## define the LSTM, dropout and fully connected layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            len(self.unique_tokens),\n",
    "            n_hidden,\n",
    "            n_layers,\n",
    "            dropout=drop_prob,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.unique_tokens))\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, hidden: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        # Stack up LSTM outputs using view. You may need to use contiguous to reshape the output.\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        ## Get the output for classification.\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(\n",
    "        self, batch_size: int, weight_device: torch.device\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Creates two new zero tensors for hidden state and cell state of LSTM\n",
    "        :param batch_size: number of sequences per batch\n",
    "        :param weight_device: torch.device(\"cuda\") for GPU init or torch.device(\"cpu\") for CPU init\n",
    "        :return: tuple of two tensors of shape [n_layers x batch_size x n_hidden]\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (\n",
    "            weight.new(self.n_layers, batch_size, self.n_hidden)\n",
    "            .zero_()\n",
    "            .to(weight_device),\n",
    "            weight.new(self.n_layers, batch_size, self.n_hidden)\n",
    "            .zero_()\n",
    "            .to(weight_device),\n",
    "        )\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unique_tokens: Tuple[str],\n",
    "        n_hidden: int = 1024,\n",
    "        n_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "        batch_size: int = 128,\n",
    "        seq_length=256,\n",
    "        lr: float = 0.001,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = CharRNN(unique_tokens, n_hidden, n_layers, drop_prob)\n",
    "        self.hidden = None\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.n_chars = len(unique_tokens)\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def training_step(\n",
    "        self, train_batch: Tuple[torch.Tensor, torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        x, y = train_batch\n",
    "        x, y = x.squeeze(0), y.squeeze(0)\n",
    "        x = one_hot_encode(x, self.n_chars)\n",
    "\n",
    "        if self.hidden is None:\n",
    "            self.hidden = self.model.init_hidden(self.batch_size, self.device)\n",
    "        self.hidden = tuple([each.data for each in self.hidden])\n",
    "\n",
    "        output, self.hidden = self.model(x, self.hidden)\n",
    "        loss = self.loss(output, y.reshape(self.batch_size * self.seq_length).long())\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IrBRlEPceEl"
   },
   "source": [
    "## Model training\n",
    "\n",
    "We'll use Adam optimizer and cross-entropy loss function. But we also have a coupe of peculiarities:\n",
    "* While cycle we'll separate hidden state from its history because a hidden state of LSTM is a tuple of hidden conitions.\n",
    "* We'll use gradient clipping to avoid \"blowing\" gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | CharRNN          | 13.0 M\n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "13.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.0 M    Total params\n",
      "52.097    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e36b4ba23074f739166f85bd982c562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "train_dataset = AnnaData(encoded, batch_size=128, seq_length=256)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,  # batching is already implemented on our side\n",
    "    shuffle=False,\n",
    "    num_workers=1,  # don't change: it will lead to the wrong implementation\n",
    ")\n",
    "# model\n",
    "char_rnn = CharRNNModule(unique_chars, n_hidden=1024, batch_size=128)\n",
    "# trainer\n",
    "trainer = pl.Trainer(max_epochs=15, gradient_clip_val=1.0, accelerator=\"gpu\", devices=1)\n",
    "trainer.fit(char_rnn, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfZxvNoDceEm"
   },
   "source": [
    "# Model applying\n",
    "\n",
    "Firstly, let's save the model to load it later. Below you can see the parameters required to making the same architecture, hyperparameters of the hidden layer and the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "q6RXl5VAceEm"
   },
   "outputs": [],
   "source": [
    "net = char_rnn.model\n",
    "checkpoint = {\n",
    "    \"n_hidden\": net.n_hidden,\n",
    "    \"n_layers\": net.n_layers,\n",
    "    \"state_dict\": net.state_dict(),\n",
    "    \"tokens\": net.unique_tokens,\n",
    "}\n",
    "\n",
    "with open(\"rnn_x_epoch.net\", \"wb\") as f:\n",
    "    torch.save(checkpoint, f)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making of predictions\n",
    "\n",
    "To predict the continuation of the text, we transmit the last character to the network, it predicts the next character, which we again transmit to the input, receive another predicted character, and so on. Our predictions are based on a categorical probability distribution over all possible characters. We can limit the number of characters at each generation step to make the resulting predicted text more intelligent by considering only some of the most likely characters. On the one hand, this approach will allow us to consider not only the most probable sequence from the point of view of the model prediction. On the other hand, we will work with a limited set of generated options, so we will get rid of completely noisy predictions. You can find more [here](https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QEIRW_B2ceEm"
   },
   "outputs": [],
   "source": [
    "def predict_next_char(\n",
    "    model: torch.nn.Module, char: str, h: torch.Tensor = None, top_k: int = None\n",
    ") -> Tuple[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Given a character and a model, predicts next character in the sequence\n",
    "    :param model: model that outputs next token probability distribution\n",
    "    :param char: last character of the sequence to continue generation from\n",
    "    :param h: hidden state of the model\n",
    "    :param top_k: number of most probable tokens to be chosen from\n",
    "    :return: tuple of next character and new hidden state\n",
    "    \"\"\"\n",
    "    # tensor inputs\n",
    "    x = torch.tensor([[model.char2int[char]]])\n",
    "    x = one_hot_encode(x, len(model.unique_tokens))\n",
    "    x = x.to(device)\n",
    "\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    out, h = model(x, h)\n",
    "\n",
    "    # get the character probabilities\n",
    "    p = torch.nn.functional.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "    # get top characters\n",
    "    if top_k is None:\n",
    "        top_ch = torch.arange(len(model.unique_tokens))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "\n",
    "    p.squeeze_()\n",
    "    top_ch.squeeze_()\n",
    "    char = top_ch[torch.multinomial(p / p.sum(), 1)]\n",
    "\n",
    "    return model.int2char[char.item()], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG38j3gQceEm"
   },
   "source": [
    "### Priming and text generation\n",
    "\n",
    "We need to define a hidden state to avoid the network to generate random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "P9vpB5gRceEm"
   },
   "outputs": [],
   "source": [
    "def sample(model, size, prime=\"The\", top_k=None):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = model.init_hidden(1, device)\n",
    "    for ch in prime:\n",
    "        char, h = predict_next_char(model, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    # pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict_next_char(model, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna that she crasped that whether series and herself thinking to him to a pondiat, that to say and a genting the best of harisa so mentilisy were\n",
      "stonding that the was trun it asked with his wife to be said as the cream of a stees on the clang and troubbed the call the head. To such a child they was so such her side in his serious complexence in the secrath of\n",
      "the sarrow, as\n",
      "he had that, to be answering thin in which she sat delighting the party of the pain, where they\n",
      "had been\n",
      "talking in the\n",
      "proppession who had telling one and so that it\n",
      "was standing work to him, and seeing the marshal offorming, and the\n",
      "perially thought of her landowned shills of the mothing, he went out and she said, would bury and horses of her\n",
      "eyes. She said to his starming something so so in his father's and his brother's side.\n",
      "\n",
      "\"I am very been it's\n",
      "so it was\n",
      "to book?\"\n",
      "\n",
      "\"The pity of all who did not bear. In the\n",
      "long\n",
      "weilent wine ther of the carreations, and tried to be delled, and the doctor and so a little more, and\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime=\"Anna\", top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "942mjdQHceEm"
   },
   "source": [
    "## Checkpoint loading and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Xt9ldUuSceEm",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said to her,\n",
      "and and they could not have been thanking a little bangs. Seeding him with the light of those, so he could not have\n",
      "said to this completening her\n",
      "and, something and the convicion.\n",
      "\n",
      "The state of the morting their carriage was shooking of the path, but his fellow had business who had standing in the clear, the\n",
      "princess. \"It's not a second with his wife, when\n",
      "she\n",
      "has now that it's so in answer, better in a tendroom,.'s\n",
      "when he stopsed all the most signer and wisting to him, the complicht had always departure all the children. The passain he had\n",
      "to do. This called to this for that seeming of the country and setting on a smold by that she changed the same again with a little back. \"I don't walt to tee it, but it said to the past family. That's ther. I am not anything, but\n",
      "I comes that he had not too went to this all that. It's a\n",
      "long on the peasing, by a shall women with a meaning of,\" said Vronsky,\n",
      "was nearer in\n",
      "a confining with the morrow that she talked at\n",
      "the capeater and this figure and staying with which without smiling.\n",
      "\n",
      "\"I'll see that his consecont teller at the sorn, and I hone,\n",
      "a cheat,\n",
      "but that? I do nitered at a minutes on a persom, thas it's\n",
      "a lund in that matter on\n",
      "the wife to say\n",
      "that I said to see you. And I said\n",
      "what hare that to breede the position, I shall don't see that in those place! They won't like it one,\" he said, and he sense of the same first camm as a stanging of which he saw him with such a particare\n",
      "of the same son, and there had\n",
      "been sonishing and the plact, and he would have said that he did not bean them all the seace, and had becouled a crowd.\n",
      "\n",
      "\"All the\n",
      "minutess and a sen and whethin was that I didn't say to the list to the subject, and has become as sometrile of it. That's way, by a chance.\"\n",
      "\n",
      "\"Annoush think the meaning in spition, and we could ton home.\" Stepan, the\n",
      "porter to think they told to the most being always still seeing himself to her, trouble to his stairs, the string holding\n",
      "of what had been asked this.\n",
      "\n",
      "\"White was al\n"
     ]
    }
   ],
   "source": [
    "with open(\"rnn_x_epoch.net\", \"rb\") as f:\n",
    "    checkpoint = torch.load(f)\n",
    "\n",
    "loaded = CharRNN(\n",
    "    checkpoint[\"tokens\"],\n",
    "    n_hidden=checkpoint[\"n_hidden\"],\n",
    "    n_layers=checkpoint[\"n_layers\"],\n",
    ")\n",
    "loaded.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "# sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sem08_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
