{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fe829b-1ed2-456f-a415-ca7544fa158f",
   "metadata": {},
   "source": [
    "# Text data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c254a09-9480-4ba2-b9d1-94311bc9316d",
   "metadata": {},
   "source": [
    "## Inrtoduction\n",
    "\n",
    "Many practical problems can include deal with test data:\n",
    "\n",
    "* Texts calssification\n",
    "  * Tone analysis (positive or negative review)\n",
    "  * Spam filtration\n",
    "  * Selection through theme or genre\n",
    "* Machine translation\n",
    "* Speech recognition and speech generation\n",
    "* Extracting information\n",
    "  * Naming of some thing (extraction of names, locations, names of organizations)\n",
    "  * Extracting of facts or events\n",
    "* Texts clasterization\n",
    "* Optical recognition of symbols\n",
    "* Orthography checking\n",
    "* Ask-answer information systems, information searching\n",
    "* Texts summarization\n",
    "* Texts generation\n",
    "\n",
    "In general we can divide the algorithm of working with text data into the following steps:\n",
    "1) Preprocessing of raw data\n",
    "2) Tokenization (making of a dictionary)\n",
    "3) Dictionary processing (deleting of stopwords and punctuation symbols)\n",
    "4) Tokens processing (lemmatization / stemming)\n",
    "5) Vectorization of texts (bag of words, TF-IDF, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f189e-6f81-412b-a572-f19549b24b4a",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "To tokenize means to divide a text into words or *tokens*. The most naive way to tokenize the text in Pythin is using of `.split()` method. But this method misses out a lot, for example, it doesn't divide punctuation and words.\n",
    "\n",
    "But in Python there is ready tokenizers of which we'll use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "205a1a62-23cf-40fb-8de8-c6e11af09846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/1.5 MB 1.3 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.2/1.5 MB 2.4 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.4/1.5 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.5/1.5 MB 3.1 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 0.8/1.5 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.1/1.5 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 4.8 MB/s eta 0:00:00\n",
      "Collecting click (from nltk)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\akalyuzhin\\workenv\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\akalyuzhin\\workenv\\lib\\site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\akalyuzhin\\workenv\\lib\\site-packages (from nltk) (4.66.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\akalyuzhin\\workenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 5.5 MB/s eta 0:00:00\n",
      "Installing collected packages: click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdeb4b9b-3e64-4d10-9090-906b6e4146ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "327dae2d-cc63-42ae-8a34-8088f930b834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "359a432e-2cf5-4494-b6b6-4c9d7ba69643",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Но не каждый может что-то исправлять:(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639c68d7-4037-4d6a-bda4-45977385323d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'может', 'что-то', 'исправлять:(']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with split\n",
    "\n",
    "example.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80900a9a-71df-4893-98a5-dbcc8ddf8ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'может', 'что-то', 'исправлять', ':', '(']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with tokenizer\n",
    "\n",
    "word_tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5145193-7e6e-4e8b-9d4b-363288b37e77",
   "metadata": {},
   "source": [
    "In `nltk` there are many tokenizers at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1bb8eac-157c-4892-8d4d-5cbb627ff0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BlanklineTokenizer',\n",
       " 'LegalitySyllableTokenizer',\n",
       " 'LineTokenizer',\n",
       " 'MWETokenizer',\n",
       " 'NLTKWordTokenizer',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'SExprTokenizer',\n",
       " 'SpaceTokenizer',\n",
       " 'StanfordSegmenter',\n",
       " 'SyllableTokenizer',\n",
       " 'TabTokenizer',\n",
       " 'TextTilingTokenizer',\n",
       " 'ToktokTokenizer',\n",
       " 'TreebankWordDetokenizer']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "dir(tokenize)[: 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9a7c5a-0b39-415c-bbde-744cc7f6c7e3",
   "metadata": {},
   "source": [
    "We can obtain indices of the beginning and of the end of each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2423b61-422f-4ce5-b77a-019de8815210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (3, 5), (6, 12), (13, 18), (19, 25), (26, 38)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_tok = tokenize.WhitespaceTokenizer()\n",
    "list(wh_tok.span_tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8485b00-7182-4a7f-8dc7-3678db26d389",
   "metadata": {},
   "source": [
    "Some tokenizers have specific behaviour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "277cf779-2a5d-4c28-b2a1-0e00d2526ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', \"n't\", 'stop', 'me']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2836014-54cb-43bd-a8e4-d393df91b626",
   "metadata": {},
   "source": [
    "And some of them not intended for using with natural languages at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c714a9c5-ecd2-4c1e-8416-f69dc288857a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(a (bc))', 'd', 'e', '(f)']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.SExprTokenizer().tokenize(\"(a (bc)) d e (f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0819e5c-4e4d-48b6-9586-aa2da1690f23",
   "metadata": {},
   "source": [
    "There is also a tokenizer useful to working with tweets and messages from social networks. It saves emojis, hashtags, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a63df2fc-3f53-42f6-9478-2a882054e067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'может', 'что-то', 'исправлять', ':(']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tw = TweetTokenizer()\n",
    "tw.tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f57f1-922d-48e3-acca-2a3c60fa3267",
   "metadata": {},
   "source": [
    "### Stopwords and punctuation\n",
    "\n",
    "Stopwords is the words frequently occurring in near all texts and not consist any important information about the test (plays the role of noise). \n",
    "That's wh they are usually deleted. The same reason punctuation are deleted, too.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9ab87eb-4ef8-41e8-ab52-abbbc4d30107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d399f0eb-7d90-44f2-8a80-7235cc99d09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7577aa08-5e81-4544-a034-68476cebf35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4844bb0-dc16-48cc-b340-03873d44bfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb0f49f2-a2e4-4a66-bab7-b72fd0838a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_rus = stopwords.words(\"russian\") + list(punctuation)\n",
    "noise_rus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4174e1b-814b-4af4-b041-4938b10d7df4",
   "metadata": {},
   "source": [
    "### Lemmatization and stemminge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82885667-5172-4bb3-860d-82a022c3270f",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "Lemmatization is process of transforming of the word to its normal form (to <b>lemma</b>).\n",
    "\n",
    "* For ouns is nominal case, singular form.\n",
    "* For adjectives is nominal case, singular form, masculine.\n",
    "* For verbs and participles is verb in infinitive form.\n",
    "\n",
    "For example, in Russian the tokens \"пью\", \"пьет\" are transformed into \"пить\". This is a good idea because:\n",
    "* Firstly, we want to consider as particular feature each word, not each form of a word.\n",
    "* Secondly, some stopwords there are in the programming libraries only in initial (normal) form and without lemmatization we drop out only this form and leave stopwords in the other forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720246a-7704-4b31-940b-7206b62d60ad",
   "metadata": {},
   "source": [
    "For the English language in `nltk` there are lemmatizers. For the Russian language there two good lemmatizers: `mystem` and `pymorphy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c816d6-4008-4c2a-b5b7-17976e9d5212",
   "metadata": {},
   "source": [
    "`mystem` has pecuilarities about its work.\n",
    "* You can download `mystem` and start in from terminal with different parameters\n",
    "* You can use the python wrapper `pymystem3` (this slower but easier way to use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b22ca458-4660-4d90-917e-c71a3f73e672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymystem3\n",
      "  Downloading pymystem3-0.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\akalyuzhin\\workenv\\lib\\site-packages (from pymystem3) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\akalyuzhin\\workenv\\lib\\site-packages (from requests->pymystem3) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\akalyuzhin\\workenv\\lib\\site-packages (from requests->pymystem3) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\akalyuzhin\\workenv\\lib\\site-packages (from requests->pymystem3) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akalyuzhin\\workenv\\lib\\site-packages (from requests->pymystem3) (2023.7.22)\n",
      "Installing collected packages: pymystem3\n",
      "Successfully installed pymystem3-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ffcc9f3-6e65-419f-a8dd-bd822e43e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "mystem_analyzer = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd307f2-9ac7-4eee-a5bb-189701a12f59",
   "metadata": {},
   "source": [
    "We've initialized of MyStem() class instance with default parameters. The following values of the fields of the class there are at all:\n",
    "\n",
    "* mystem_bin - path to `mystem` if there are several\n",
    "* grammar_info - whether we need grammatical information or we need only lemms (need grammatic information in default)\n",
    "* disambiguation - whether we need resolving of homonymy (yes in default)\n",
    "* entire_input - whether is need saving everything in output (spaces, for instance) or it is acceptable to drop out something (in default everythin is saved).\n",
    "\n",
    "MyStem methods take a string on input, the tokenizer is inside. We can analyze by words but in that case the tokenizer can't account the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12eb81e7-f427-42e1-a823-2daa2ba30f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['но', ' ', 'не', ' ', 'каждый', ' ', 'мочь', ' ', 'что-то', ' ', 'исправлять', ':(\\n']\n"
     ]
    }
   ],
   "source": [
    "print(mystem_analyzer.lemmatize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e58449-25e2-4aab-943d-9f9b6e7ed92f",
   "metadata": {},
   "source": [
    "`pymorphy` is another lemmatizer for the Russian language. This is python module, quite fast and have a lot of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aff9f239-0b08-447a-b2c2-4d37d9a801c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 0.0/55.5 kB ? eta -:--:--\n",
      "     ------------------------------------ --- 51.2/55.5 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 55.5/55.5 kB 962.6 kB/s eta 0:00:00\n",
      "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "     ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.2/8.2 MB 4.6 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.5/8.2 MB 4.8 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.7/8.2 MB 4.9 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.2/8.2 MB 6.1 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 1.4/8.2 MB 6.2 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 1.6/8.2 MB 5.8 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 1.9/8.2 MB 5.9 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 2.4/8.2 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 2.5/8.2 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 2.7/8.2 MB 5.8 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 3.0/8.2 MB 5.8 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 3.2/8.2 MB 5.8 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 3.2/8.2 MB 5.5 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 3.6/8.2 MB 5.5 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 3.8/8.2 MB 5.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 4.0/8.2 MB 5.3 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 4.1/8.2 MB 5.1 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 4.2/8.2 MB 5.0 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.4/8.2 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 4.6/8.2 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 4.7/8.2 MB 4.8 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.9/8.2 MB 4.7 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 5.0/8.2 MB 4.6 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 5.2/8.2 MB 4.6 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 5.3/8.2 MB 4.5 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 5.5/8.2 MB 4.5 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 5.6/8.2 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 5.8/8.2 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 5.9/8.2 MB 4.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 6.1/8.2 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 6.3/8.2 MB 4.3 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 6.4/8.2 MB 4.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 6.6/8.2 MB 4.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 6.8/8.2 MB 4.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 6.9/8.2 MB 4.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 7.1/8.2 MB 4.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 7.3/8.2 MB 4.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 7.5/8.2 MB 4.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 7.6/8.2 MB 4.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 7.8/8.2 MB 4.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 8.0/8.2 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.2/8.2 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.2/8.2 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 8.2/8.2 MB 4.1 MB/s eta 0:00:00\n",
      "Collecting docopt>=0.6 (from pymorphy2)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13772 sha256=99d4373187f3f251da508039d28c1b6269924b603c4b7e8f46559a37acffaa63\n",
      "  Stored in directory: c:\\users\\akalyuzhin\\appdata\\local\\pip\\cache\\wheels\\fc\\ab\\d4\\5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
      "Collecting pymorphy2-dicts\n",
      "  Downloading pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1 MB)\n",
      "     ---------------------------------------- 0.0/7.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/7.1 MB 1.6 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.2/7.1 MB 3.0 MB/s eta 0:00:03\n",
      "     -- ------------------------------------- 0.4/7.1 MB 3.3 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 0.8/7.1 MB 4.8 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.0/7.1 MB 4.7 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.0/7.1 MB 4.1 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.0/7.1 MB 4.1 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.0/7.1 MB 4.1 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.0/7.1 MB 4.1 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 1.2/7.1 MB 2.6 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 1.6/7.1 MB 3.1 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 1.9/7.1 MB 3.6 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 2.0/7.1 MB 3.5 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 2.1/7.1 MB 3.3 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 2.4/7.1 MB 3.4 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 2.5/7.1 MB 3.5 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 2.8/7.1 MB 3.5 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 3.0/7.1 MB 3.6 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 3.1/7.1 MB 3.7 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 3.1/7.1 MB 3.7 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 3.1/7.1 MB 3.7 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 3.1/7.1 MB 3.7 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 3.1/7.1 MB 3.7 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 3.1/7.1 MB 3.7 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 3.3/7.1 MB 2.8 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 3.5/7.1 MB 2.9 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 3.8/7.1 MB 3.0 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 4.0/7.1 MB 3.1 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 4.2/7.1 MB 3.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.2/7.1 MB 3.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.2/7.1 MB 3.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.2/7.1 MB 3.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.2/7.1 MB 3.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.2/7.1 MB 3.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.2/7.1 MB 3.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 4.4/7.1 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 4.6/7.1 MB 2.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 4.8/7.1 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 5.1/7.1 MB 2.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 5.2/7.1 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 5.3/7.1 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 5.6/7.1 MB 2.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 5.9/7.1 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 6.1/7.1 MB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 6.3/7.1 MB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 6.3/7.1 MB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 6.3/7.1 MB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 6.3/7.1 MB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 6.3/7.1 MB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 6.3/7.1 MB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 6.3/7.1 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 6.6/7.1 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 6.9/7.1 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.1/7.1 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.1/7.1 MB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pymorphy2-dicts\n",
      "Successfully installed pymorphy2-dicts-2.4.393442.3710985\n",
      "Requirement already satisfied: DAWG-Python in c:\\users\\akalyuzhin\\workenv\\lib\\site-packages (0.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2\n",
    "!pip install pymorphy2-dicts\n",
    "!pip install DAWG-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "412ace9f-202a-47a5-bc79-77590f370c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8efa7c2d-2930-4351-8a60-93fc89d83741",
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy2_analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc488d-e73b-428d-9eed-0fea6b4fdfb2",
   "metadata": {},
   "source": [
    "`pymorphy2` deals with particular words, not with strings (in difference to `mystem`)\n",
    "\n",
    "MorphAnalyzer.parse() take a word and returns possible morphemes.\n",
    "\n",
    "Each word has a tag. Tag is set of grammems characterising the word. For instance, the \"VERB,perf,plur,past,indc\" means that the word is verb, have perfect form, intransitive, have past tense and have indicative mood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ed8d1ef-1e80-44c7-a1c6-1155ea9d8b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='хочет', tag=OpencorporaTag('VERB,impf,tran sing,3per,pres,indc'), normal_form='хотеть', score=1.0, methods_stack=((DictionaryAnalyzer(), 'хочет', 3136, 5),))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana = pymorphy2_analyzer.parse(\"хочет\")\n",
    "ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "179107ab-79aa-498a-9c94-ffb16a1ffce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хотеть'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d299bf8-5772-473d-a292-0e404cb4637a",
   "metadata": {},
   "source": [
    "`mystem` vs `pymorphy`\n",
    "1) With `mystem` we should use Mac OS or Linux OS because with Windows `mystem` works super slowly if your text is large.\n",
    "2) But `mustem` can resolve homonymy by context (although not always successfully), and `pymorphy2` takes a word on input and therefore can't use the context, and can't resolve homonymy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00118ac8-98ef-4bc9-88e7-8b7564bab95c",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Stemming is a process of discarding affixes (suffixes or endings) does not have to lead making of forms of words existing in the language.\n",
    "\n",
    "In `nltk` there is `snowball` module with stemming algorithms. The stemming algorithms choosing according to used language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee509c5a-d2d7-4e98-9124-0ddec2229217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f578c120-f971-4132-913c-1c84b9da8c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = word_tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9380fdf-8ec8-41f5-a3d2-33b4879192be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "но не кажд может что-т исправля : (\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")\n",
    "stemmed_example = [stemmer.stem(w) for w in tokenized_example]\n",
    "print(\" \".join(stemmed_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad480fa-6113-48fd-b4e0-8f32a43c14c4",
   "metadata": {},
   "source": [
    "Or for the English language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20c9bdf8-4e06-4e89-8685-d22386c3f5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\n",
      "====================\n",
      "['in', 'my', 'younger', 'and', 'more', 'vulnerable', 'years', 'my', 'father', 'gave', 'me', 'some', 'advice', 'that', 'I', 'been', 'turning', 'over', 'in', 'my', 'mind', 'ever', 'since']\n"
     ]
    }
   ],
   "source": [
    "text = \"in my younger and more vulnerable years my father gave me some advice that I\\'ve been turning over in my mind ever since.\"\n",
    "print(text)\n",
    "text_tokenized = [w for w in word_tokenize(text) if w.isalpha()]\n",
    "print(\"====================\")\n",
    "print(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be8c16ae-c645-42aa-ad15-28404df93669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in my younger and more vulner year my father gave me some advic that i been turn over in my mind ever sinc\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "text_stemmed = [stemmer.stem(w) for w in text_tokenized]\n",
    "print(\" \".join(text_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b30391-e1a1-4b15-a787-ebd02a9a0edc",
   "metadata": {},
   "source": [
    "### Bag-of-words and TF-IDF\n",
    "\n",
    "But we must work with ML models which use numbers, not letters or strings. So, we need some methods to transform words/sentences into number. In other words, let's consider methods of *vectorization* of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b19d98-66f8-4cba-856a-1798d82ddc38",
   "metadata": {},
   "source": [
    "#### Bag-of-words\n",
    "\n",
    "Let us to have texts collection $D = \\{d_i\\}_{i=1}^{\\ell}$ and dictionary of all the words in the collection $V = \\{\\upupsilon\\}_{j=1}^{d}$. In this case some text $d_i$ describes by a vector $(x_{ij})_{j=1}^{d}$ where:\n",
    "$$x_{ij} = \\sum_{\\upupsilon \\in d_i} [\\upupsilon = \\upupsilon_j]$$\n",
    "\n",
    "Thus, the text $d_i$ describes by the vector of the number of occurrences of each word from the dictionary in the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13d98431-f55e-451f-b4e5-db9b3dcb52c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I like my cat\",\n",
    "    \"My cat is the most perfect cat\",\n",
    "    \"Is this cat or is this bread\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b884245-73e0-49eb-8f8e-55f3502f5c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I like my cat',\n",
       " 'My cat is the most perfect cat',\n",
       " 'Is this cat or is this bread']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_tokenized = [\n",
    "    \" \".join([w for w in word_tokenize(t) if w.isalpha()]) for t in texts\n",
    "]\n",
    "texts_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ffa9784-88e0-4eff-8cad-62a3b3c923f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cnt_vec = CountVectorizer()\n",
    "X = cnt_vec.fit_transform(texts_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4080974e-73a5-48e3-aa8a-ed4bf9448130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['like', 'my', 'cat', 'is', 'the', 'most', 'perfect', 'this', 'or', 'bread'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vec.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "880f98fa-bb9a-40f9-9598-fb30284fbcb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d5b2a34-0341-4322-bac7-0847c67d72a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 2, 1, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [1, 1, 2, 0, 0, 0, 1, 0, 0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e0782-48e9-41c1-a232-f4519313cb14",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "Note that in Bag-of-words method if a word is frequently occurred in one text but almost never in another one, this word has the same weight as the words which occurred frequently in all the texts.\n",
    "To fix that we can use TF-IDF statistical method, characterising importance of a word for particular text from the base of texts. Firstly we must calculate for each word from text $d$ relative frequency of occurrence in this text (**T**erm **F**requency):\n",
    "$$TF(t, d) = \\dfrac{C(t|d)}{\\sum_{k \\in d} C(k|d)}$$\n",
    "where $C(t|d)$ is the number of occurence of word $t$ in the text $d$.\n",
    "\n",
    "Also we must calculate for each word from text $d$ reversal frequency of occurence in the whole texts corpus $D$ (**I**nverse **D**ocument **F**requency):\n",
    "$IDF(t, d) = \\log(\\dfrac{|D|}{|\\{ d_i \\in D | t \\in d_i\\}|})$\n",
    "where $|D|$ is the size of  texts corpus, $|\\{ d_i \\in D | t \\in d_i\\}|$ - number of the documents where the word $t$ there is.\n",
    "\r",
    "Llogarith here need to decrease of scale of the weights (sometimes in texts corpuses there are so many texts).\n",
    "\n",
    "In summary we can assign a weight to each word $t$ from the text $d$:\n",
    "$$TF-IDF(t, d, D) = TF(t, d) \\cdot IDF(t, d)$$\n",
    "\n",
    "Interpretation of the formula is: more frequently a word is occurred in the given text and the more rarely in all another ones, more important this word for this particular text.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "263eb4f0-8dc7-4db1-b014-b24072f8c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(texts_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4a3a003-4e40-41fc-9ad9-0fef428b2190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['like', 'my', 'cat', 'is', 'the', 'most', 'perfect', 'this', 'or', 'bread'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce9774ec-089b-46c6-b7c8-34c1d278ef1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d2a7d75-9783-43d0-81c9-46b8ddf1beec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.42544054, 0.        , 0.72033345, 0.        ,\n",
       "        0.54783215, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.50130994, 0.32276391, 0.        , 0.42439575,\n",
       "        0.32276391, 0.        , 0.42439575, 0.42439575, 0.        ],\n",
       "       [0.33976626, 0.20067143, 0.516802  , 0.        , 0.        ,\n",
       "        0.        , 0.33976626, 0.        , 0.        , 0.67953252]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d9b3d1-54a2-4242-99be-d0b1f03fc9e9",
   "metadata": {},
   "source": [
    "## Solving of a problem with text data\n",
    "\n",
    "We'll solve here a problem of tweets classification by its tone. Let's take a dataset where we knows what emotional tone each tweet have - positive or negative (taken from dropbox.com). The problem is predict emotional tone, binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ebb00a9-f49b-4da5-9214-1f71604cd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8fa6a393-7e7f-4ea6-b37a-1b53c74ff72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.read_csv(\"positive.csv\", sep=\";\", usecols=[3], names=[\"text\"])\n",
    "positive[\"label\"] = \"positive\"\n",
    "negative = pd.read_csv(\"negative.csv\", sep=\";\", usecols=[3], names=[\"text\"])\n",
    "negative[\"label\"] = \"negative\"\n",
    "df = pd.concat([positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23cbad0e-a0ee-43b3-bf4f-299ef9de89a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111918</th>\n",
       "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111919</th>\n",
       "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111920</th>\n",
       "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111921</th>\n",
       "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111922</th>\n",
       "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     label\n",
       "111918  Но не каждый хочет что то исправлять:( http://...  negative\n",
       "111919  скучаю так :-( только @taaannyaaa вправляет мо...  negative\n",
       "111920          Вот и в школу, в говно это идти уже надо(  negative\n",
       "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...  negative\n",
       "111922  Такси везет меня на работу. Раздумываю приплат...  negative"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afe9bdac-4e22-4fb9-8e41-31d2f98cb711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226834, 2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05ec850a-5b55-4517-a24d-ca378084d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.label, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485b53a-2993-44d1-87de-09f66a21d3ee",
   "metadata": {},
   "source": [
    "**n-grams**\n",
    "\n",
    "n-grams are sequences of n neighbour tokens from initial text. In the simplest case it may be sequences of letters or words.\n",
    "\n",
    "n-grams provides information about n neighbour tokens of some token. May be useful in problems solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9cf20b9-cefc-4668-b157-5cf1355b9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0cd0cbf-6aca-4e86-b01d-be22e58699f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Если б мне платили каждый раз\".split()\n",
    "list(ngrams(sent, 1)) # unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81e3590b-99b8-4d97-a95e-66ee51634f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б'),\n",
       " ('б', 'мне'),\n",
       " ('мне', 'платили'),\n",
       " ('платили', 'каждый'),\n",
       " ('каждый', 'раз')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 2)) # bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1a06de6b-7ed4-4b54-b276-784d351bbe28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне'),\n",
       " ('б', 'мне', 'платили'),\n",
       " ('мне', 'платили', 'каждый'),\n",
       " ('платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 3)) # trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3915636f-ba99-4447-bc29-426645603aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
       " ('б', 'мне', 'платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 5)) # or even like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48661c0b-e64e-4e36-b57a-06a6c11cb194",
   "metadata": {},
   "source": [
    "As an alternative, we can use `CountVectorizer` which wors the following way:\n",
    "* Build for each document (each string given) a vector of dimension of number of tokens in our dictionary\n",
    "* Fill each $i$ element with number of occurrences of token in given document.\n",
    "\n",
    "`ngram_range` parameter is responsible for what n-grams we use as features.\n",
    "* `ngram_range = (1, 1)` - unigrams\n",
    "* `ngram_range = (3, 3)`  - trigrams\n",
    "* `ngram_range = (1, 3)`  - unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321cf04-5571-4f33-9bf0-a61295ddf6e9",
   "metadata": {},
   "source": [
    "Now let's train the first baseline - log reg on unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6570b36e-dc1f-46ab-b633-d42dee66ccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.77      0.76     27957\n",
      "    positive       0.77      0.76      0.77     28752\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range = (1, 1))\n",
    "bow = vec.fit_transform(X_train) # bow - bag of words\n",
    "bow_test = vec.transform(X_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(bow_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8bfd2a-ca17-420a-a5a9-e54271663b81",
   "metadata": {},
   "source": [
    "The same for trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "532ef7c3-8cf7-4090-8179-79593b8d28ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.47      0.57     27957\n",
      "    positive       0.61      0.82      0.70     28752\n",
      "\n",
      "    accuracy                           0.65     56709\n",
      "   macro avg       0.67      0.65      0.64     56709\n",
      "weighted avg       0.67      0.65      0.64     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range = (3, 3))\n",
    "bow = vec.fit_transform(X_train) # bow - bag of words\n",
    "bow_test = vec.transform(X_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(bow_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddd7be9-b4e3-4e87-bb79-7f565520db7d",
   "metadata": {},
   "source": [
    "Trigrams worse here, as you can see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24fb8ac-a5d6-43b8-9c07-2545f77e022c",
   "metadata": {},
   "source": [
    "Now let's repeat for unigrams with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d3f3f2b6-f4fe-47aa-bd0a-e21372acfe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.75      0.76     27957\n",
      "    positive       0.76      0.78      0.77     28752\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(ngram_range = (1, 1))\n",
    "bow = vec.fit_transform(X_train) # bow - bag of words\n",
    "bow_test = vec.transform(X_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(bow_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841771d9-434a-4f4b-b0de-a2874445febe",
   "metadata": {},
   "source": [
    "**Explorative analysis**\n",
    "\n",
    "Sometimes, in some problems, punctuation isn't a noise. For our example, what if we don't remove punctuation at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4525b0a8-8afa-472a-be72-bf5d3e806cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.97      0.96     27957\n",
      "    positive       0.97      0.95      0.96     28752\n",
      "\n",
      "    accuracy                           0.96     56709\n",
      "   macro avg       0.96      0.96      0.96     56709\n",
      "weighted avg       0.96      0.96      0.96     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range = (1, 1), tokenizer=word_tokenize)\n",
    "bow = vec.fit_transform(X_train) # bow - bag of words\n",
    "bow_test = vec.transform(X_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(bow_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab02b4d-6e3f-482d-9f43-9596b0587ecb",
   "metadata": {},
   "source": [
    "You can see significant increasing of quality. This is because among punctuation symbols were very significant for the text corpus (with large weights). Let's find out what symbols it were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cdd371ec-9b67-4a4d-82fe-a4e0038b6860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "')'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_vocab = {value: key for key, value in vec.vocabulary_.items()}\n",
    "reverse_vocab[np.argmax(clf.coef_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8535c79b-76c8-4171-a01f-3e22ac826e0a",
   "metadata": {},
   "source": [
    "Let's see now how token with large weight (very important token) do classification without any machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "66026277-32fe-4857-8e42-e99b17c1076d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92     27957\n",
      "    positive       1.00      0.83      0.91     28752\n",
      "\n",
      "    accuracy                           0.91     56709\n",
      "   macro avg       0.93      0.92      0.91     56709\n",
      "weighted avg       0.93      0.91      0.91     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cool_token = reverse_vocab[np.argmax(clf.coef_)]\n",
    "pred = [\"positive\" if cool_token in tweet else \"negative\" for tweet in X_test]\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d23e92-5614-4ff1-94cf-0644387306df",
   "metadata": {},
   "source": [
    "**Symbolic n-grams**\n",
    "\n",
    "We can also use in machine learning problems with texts char objects as features. Let's try with unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "81fb6946-c5ab-4af1-8706-37dcc37b7014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.97      0.98     27957\n",
      "    positive       0.98      0.99      0.98     28752\n",
      "\n",
      "    accuracy                           0.98     56709\n",
      "   macro avg       0.98      0.98      0.98     56709\n",
      "weighted avg       0.98      0.98      0.98     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range = (1, 1), analyzer=\"char\")\n",
    "bow = vec.fit_transform(X_train) # bow - bag of words\n",
    "bow_test = vec.transform(X_test)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "bow = scaler.fit_transform(bow)\n",
    "bow_test = scaler.transform(bow_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(bow_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a536a04-da40-4377-a3bb-d22481c761fa",
   "metadata": {},
   "source": [
    "Such a result here because in our example we have punctuation symbol (char object in fact) as a token with the largest weight and, as a consequence, with the largest importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c213a73-eeaf-461f-a5ef-c9df63d2582b",
   "metadata": {},
   "source": [
    "Another one wonderful pecuilarity of char features is they no need tokenization and lemmarization. So, we can use such an approach in problems with languages for which there are no any analyzers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d54e64-95ad-46a9-b139-a13e543e20fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
